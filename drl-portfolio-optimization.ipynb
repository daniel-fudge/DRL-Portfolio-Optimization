{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Portfolio Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "This workbook is the culmination of three separate half credit independent studies by the author [Daniel Fudge](https://www.linkedin.com/in/daniel-fudge) with [Professor Yelena Larkin](https://www.linkedin.com/in/yelena-larkin-6b7b361b/) \n",
    "as part of a concurrent Master of Business Administration (MBA) and a [Diploma in Financial Engineering](https://schulich.yorku.ca/programs/fnen/) from the [Schulich School of Business](https://schulich.yorku.ca/).  I wanted to thank Schulich and especially Professor Larkin for giving me the freedom to explorer the intersection of Machine Learning and Finance.   \n",
    "\n",
    "I'd like to also mention the excellent training I recieved from [A Cloud Guru](https://acloud.guru/learn/aws-certified-machine-learning-specialty) to prepare for this project.  \n",
    "\n",
    "This notebook takes much of the design and low level code from the AWS sample portfolio management [Notebook](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_portfolio_management_coach_customEnv), which inturn is based on Jiang, Zhengyao, Dixing Xu, and Jinjun Liang. \"A deep reinforcement learning framework for the financial portfolio management problem.\" arXiv preprint arXiv:1706.10059 (2017).    \n",
    "\n",
    "As detailed below, this notebook relies on the [RL Coach](https://github.com/NervanaSystems/coach#batch-reinforcement-learning) from Intel AI Labs, Apache [MXNet](https://mxnet.apache.org/) and OpenAI [Gym](https://gym.openai.com/).  All of which are amazing projects that I highly recommend investigating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Now that we have the signals `signals.pkl` and prices `stock-data-clean.pkl` processed we can build the Reinforcement Learning algorithm.  The signals will be used to define the state of the market environment, called `observations` in the Gym documentation and `state` in other locations.  The prices will be used to generate the `rewards`.   \n",
    "\n",
    "For a refresher on DRL I recommend looking through [report2](docs/report2.pdf) in this repo.  The basic RL feed back loop is shown below.    \n",
    "\n",
    "![RL](docs/rl.png \"Reinforcement Learning Loop\")\n",
    "\n",
    "Reinforcement Learning feedback loop.    \n",
    "Image source: https://i.stack.imgur.com/eoeSq.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "The `action` space is simply vector containing the weights of the stocks in the portfolio.  Inside the environment these weights are limited to (0, 1) and an addition weight is added for the amount of cash in the portfolio.  The cash weight is simply one minus the sum of the other weights and also limited to (0, 1).  The last step is to normalize these weight so the sum is equal to 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State (or Observation)\n",
    "The `state` is simply the signals we compiled in the previous data preparation [notebook](docs/data-preparation-no-memory.ipynb) including a time embbeding called the `window_length` within the code.  Instead of using the LSTM discussed previously, we are using a short Convolutional Neural Net (CNN) to capture some historical information from the data within the `agent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "For the agent we are using the [RL Coach](https://github.com/NervanaSystems/coach#batch-reinforcement-learning) from Intel AI Labs that is integrated into AWS [Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/reinforcement-learning.html#sagemaker-rl).  As you can see below there are a large number of RL algorithms to choose from.  In  [report2](docs/report2.pdf) we discussed serval of these and focused on the Deep Deterministic Policy Gradient (DDPG) Actor-Critic method however for this test we are trying the Proximal Policy Optimization (PPO) algorithm that is explained nicely by Jonathan Hui [here](https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12) and the original paper by Schulman et al. [here](https://arxiv.org/pdf/1707.06347.pdf).    \n",
    "\n",
    "Remember that the goal of the agent is to learn a policy that maximizes the sum of discounted rewards, which in our case will result in the maximization of the portfolio value.\n",
    "\n",
    "![Coach](docs/rl-coach.png)\n",
    "RL Coach Algorithms\n",
    "Imafe Source:  https://github.com/NervanaSystems/coach#batch-reinforcement-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweaks to Agent\n",
    "If you are playing with the number of signals you may have to change the input to the Agent.  Unfortunately one of the disadvantages of using pre-built frameworks is the extra overhead and the difficulity finding the little details you need to tweak.   \n",
    "\n",
    "Here we have to define input, which for us is a 2D CNN or Conv2D as a RL Coach [input embedder](https://nervanasystems.github.io/coach/design/network.html) but it is actually passed to the Conv2D within Apache [MXNet].   The RL Coach [code](https://github.com/NervanaSystems/coach/blob/master/rl_coach/architectures/layers.py) that passes the arguments to the MXNet Conv2D [code](https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.nn.Conv2D.html) only accepts the 3 arguments and the MXNet infers the rest.  This lack of control makes setting up the problem a little tricky.  Also for time series data the MXNet [Conv1D](https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.nn.Conv1D.html) would be more appropriate but RL coach doesn't give us the flexibility.    \n",
    "\n",
    "The interface to RL Coach is defined in `preset-portfolio-management-clippedppo.py` shown below.  The line you may want to tweak is under \"Agent\" and contains the Conv2D argument.  This needs to coorespond to the observation space definition and how the observation is pulled from the signals in the `portfolio_env.py` file shown below as well.  Both of these are in the `src` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.agents.clipped_ppo_agent\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClippedPPOAgentParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.architectures.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense, Conv2d\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m VisualizationParameters, PresetValidationParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MiddlewareScheme, DistributedCoachSynchronizationType\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.core_types\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, RunPhase\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.environments.gym_environment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GymVectorEnvironment, ObservationSpaceType\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.exploration_policies.e_greedy\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m EGreedyParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.basic_rl_graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BasicRLGraphManager\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ScheduleParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.schedules\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LinearSchedule\r\n",
      "\r\n",
      "\u001b[37m####################\u001b[39;49;00m\r\n",
      "\u001b[37m# Graph Scheduling #\u001b[39;49;00m\r\n",
      "\u001b[37m####################\u001b[39;49;00m\r\n",
      "\r\n",
      "schedule_params = ScheduleParameters()\r\n",
      "schedule_params.improve_steps = TrainingSteps(\u001b[34m60000\u001b[39;49;00m)\r\n",
      "schedule_params.steps_between_evaluation_periods = EnvironmentSteps(\u001b[34m2048\u001b[39;49;00m)\r\n",
      "schedule_params.evaluation_steps = EnvironmentEpisodes(\u001b[34m5\u001b[39;49;00m)\r\n",
      "schedule_params.heatup_steps = EnvironmentSteps(\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m#########\u001b[39;49;00m\r\n",
      "\u001b[37m# Agent #\u001b[39;49;00m\r\n",
      "\u001b[37m#########\u001b[39;49;00m\r\n",
      "\r\n",
      "agent_params = ClippedPPOAgentParameters()\r\n",
      "\r\n",
      "\u001b[37m# Input Embedder with no CNN\u001b[39;49;00m\r\n",
      "\u001b[37m#agent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = [Dense(71)]\u001b[39;49;00m\r\n",
      "\u001b[37m#agent_params.network_wrappers['main'].input_embedders_parameters['observation'].activation_function = 'tanh'\u001b[39;49;00m\r\n",
      "\u001b[37m#agent_params.network_wrappers['main'].middleware_parameters.scheme = [Dense(128)]\u001b[39;49;00m\r\n",
      "\u001b[37m#agent_params.network_wrappers['main'].middleware_parameters.activation_function = 'tanh'\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Input Embedder used in sample notebook\u001b[39;49;00m\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].input_embedders_parameters[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].scheme = [Conv2d(\u001b[34m32\u001b[39;49;00m, [\u001b[34m3\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m], \u001b[34m1\u001b[39;49;00m)]\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].middleware_parameters.scheme = MiddlewareScheme.Empty\r\n",
      "\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].learning_rate = \u001b[34m0.0001\u001b[39;49;00m\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].batch_size = \u001b[34m64\u001b[39;49;00m\r\n",
      "agent_params.algorithm.clipping_decay_schedule = LinearSchedule(\u001b[34m1.0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m, \u001b[34m150000\u001b[39;49;00m)\r\n",
      "agent_params.algorithm.discount = \u001b[34m0.99\u001b[39;49;00m\r\n",
      "agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(\u001b[34m2048\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m# Distributed Coach synchronization type.\u001b[39;49;00m\r\n",
      "agent_params.algorithm.distributed_coach_synchronization_type = DistributedCoachSynchronizationType.SYNC\r\n",
      "\r\n",
      "agent_params.exploration = EGreedyParameters()\r\n",
      "agent_params.exploration.epsilon_schedule = LinearSchedule(\u001b[34m1.0\u001b[39;49;00m, \u001b[34m0.01\u001b[39;49;00m, \u001b[34m10000\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m###############\u001b[39;49;00m\r\n",
      "\u001b[37m# Environment #\u001b[39;49;00m\r\n",
      "\u001b[37m###############\u001b[39;49;00m\r\n",
      "\r\n",
      "env_params = GymVectorEnvironment(level=\u001b[33m'\u001b[39;49;00m\u001b[33mportfolio_env:PortfolioEnv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "env_params.\u001b[31m__dict__\u001b[39;49;00m[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation_space_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = ObservationSpaceType.Tensor\r\n",
      "\r\n",
      "\u001b[37m########\u001b[39;49;00m\r\n",
      "\u001b[37m# Test #\u001b[39;49;00m\r\n",
      "\u001b[37m########\u001b[39;49;00m\r\n",
      "\r\n",
      "preset_validation_params = PresetValidationParameters()\r\n",
      "preset_validation_params.test = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "\r\n",
      "graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\r\n",
      "                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\r\n",
      "                                    preset_validation_params=preset_validation_params)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/preset-portfolio-management-clippedppo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "This leaves us with the environment to generate.  Here we build a custom finacial market environment (or simulator) based on the signals and prices we compiled previously on top of the OpenAI [Gym](https://gym.openai.com/), which is also integrated into AWS [Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-rl-environments.html#sagemaker-rl-environments-gym).  This takes care of the low level integration allows us to focus on the features of the environment specific to portfolio optimization.   \n",
    "\n",
    "During each training epoch, the trainer randomly selects a date to start.  It then steps through each day in the epoch and following these high level operations.\n",
    "1. Initialze the state from the signals with the randomly selected start date and sets the portfolio to $1 of cash.\n",
    "1. Pass the state to the agent who generates an action, which is a new set of desired portfolio weights.\n",
    "1. Pass the new weigths (action) to the environment who calculates: \n",
    "  * The new portfolio value based on changes in the prices, weights and transaction costs.   \n",
    "  * The reward based on the previous weights and the change in prices.\n",
    "  * The new state, which is simply pulled from the signals dataset.\n",
    "1. Pass the new reward and state to the agent, who must then learn to make a better action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Portfolio Environment\n",
    "The code below implements the custom financial market environment that the agent must trade in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\" Modified from https://github.com/awslabs/amazon-sagemaker-examples \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym.spaces\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\r\n",
      "EPS = \u001b[34m1e-8\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mPortfolioEnv\u001b[39;49;00m(gym.Env):\r\n",
      "    \u001b[33m\"\"\" This class creates the financial market environment that the Agent interact with.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    It extends the OpenAI Gym environment https://gym.openai.com/.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    More information of how it is integrated into AWS is found here\u001b[39;49;00m\r\n",
      "\u001b[33m    https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-rl-environments.html#sagemaker-rl-environments-gym\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    The observations include a history of the signals with the given `window_length` ending at the current date.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        steps (int):  Steps or days in an episode.\u001b[39;49;00m\r\n",
      "\u001b[33m        trading_cost (float):  Cost of trade as a fraction.\u001b[39;49;00m\r\n",
      "\u001b[33m        window_length (int):  How many past observations to return.\u001b[39;49;00m\r\n",
      "\u001b[33m        start_date_index (int):  The date index in the signals and price arrays.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Attributes:\u001b[39;49;00m\r\n",
      "\u001b[33m        action_space (gym.spaces.Box):  [n_tickers]  The portfolio weighting not including cash.\u001b[39;49;00m\r\n",
      "\u001b[33m        dates (np.array of np.datetime64):  [n_days] Dates for the signals and price history arrays.\u001b[39;49;00m\r\n",
      "\u001b[33m        info_list (list):  List of info dictionaries for each step.\u001b[39;49;00m\r\n",
      "\u001b[33m        n_signals (int):  Number of signals in each observation.\u001b[39;49;00m\r\n",
      "\u001b[33m        n_tickers (int):  Number of tickers in the price history.\u001b[39;49;00m\r\n",
      "\u001b[33m        observation_space (gym.spaces.Box)  [self.n_signals, window_length]  The signals with a window_length history.\u001b[39;49;00m\r\n",
      "\u001b[33m        portfolio_value (float):  The portfolio value, starting with $1 in cash.\u001b[39;49;00m\r\n",
      "\u001b[33m        gain (np.array):  [n_days, n_tickers, gain] The relative price vector; today's / yesterday's price.\u001b[39;49;00m\r\n",
      "\u001b[33m        signals (np.array):  [n_signals, n_days, 1]  Signals that define the observable environment.\u001b[39;49;00m\r\n",
      "\u001b[33m        start_date_index (int):  The date index in the signals and price arrays.\u001b[39;49;00m\r\n",
      "\u001b[33m        step_number (int):  The step number of the episode.\u001b[39;49;00m\r\n",
      "\u001b[33m        steps (int):  Steps or days in an episode.\u001b[39;49;00m\r\n",
      "\u001b[33m        tickers (list of str):  The stock tickers.\u001b[39;49;00m\r\n",
      "\u001b[33m        trading_cost (float):  Cost of trade as a fraction.\u001b[39;49;00m\r\n",
      "\u001b[33m        window_length (int):  How many past observations to return.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, steps=\u001b[34m506\u001b[39;49;00m, trading_cost=\u001b[34m0.0025\u001b[39;49;00m, window_length=\u001b[34m5\u001b[39;49;00m, start_date_index=\u001b[34m4\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"An environment for financial portfolio management.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# Initialize some local parameters\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.csv = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/output/data/portfolio-management.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.info_list = \u001b[36mlist\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.portfolio_value = \u001b[34m1.0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.step_number = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.steps = steps\r\n",
      "        \u001b[36mself\u001b[39;49;00m.trading_cost = trading_cost\r\n",
      "\r\n",
      "        \u001b[37m# Save some arguments as attributes\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.window_length = window_length\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_date_index = start_date_index\r\n",
      "\r\n",
      "        \u001b[37m# Read the stock data and convert to the relative price vector (gain)\u001b[39;49;00m\r\n",
      "        \u001b[37m#   Note the raw prices have an extra day vs the signals to calculate gain\u001b[39;49;00m\r\n",
      "        raw_prices = pd.read_csv(os.path.join(os.path.dirname(\u001b[31m__file__\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mstock-data-clean.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),index_col=\u001b[34m0\u001b[39;49;00m, parse_dates=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.tickers = raw_prices.columns.tolist()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.gain = np.hstack((np.ones((raw_prices.shape[\u001b[34m0\u001b[39;49;00m]-\u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)), raw_prices.values[\u001b[34m1\u001b[39;49;00m:] / raw_prices.values[:-\u001b[34m1\u001b[39;49;00m]))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.dates = raw_prices.index.values[\u001b[34m1\u001b[39;49;00m:]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.n_dates = \u001b[36mself\u001b[39;49;00m.dates.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.n_tickers = \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.tickers)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.weights = np.insert(np.zeros(\u001b[36mself\u001b[39;49;00m.n_tickers), \u001b[34m0\u001b[39;49;00m, \u001b[34m1.0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# The start index must >= the window length to avoid a negative index\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_date_index = \u001b[36mmax\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.start_date_index, \u001b[36mself\u001b[39;49;00m.window_length - \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# The start index <= n_date - the steps in the episode to avoid over running the price history\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_date_index = \u001b[36mmin\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.start_date_index, \u001b[36mself\u001b[39;49;00m.n_dates - \u001b[36mself\u001b[39;49;00m.steps)\r\n",
      "\r\n",
      "        \u001b[37m# Read the signals\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.signals = pd.read_csv(os.path.join(os.path.dirname(\u001b[31m__file__\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33msignals.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "                                   index_col=\u001b[34m0\u001b[39;49;00m, parse_dates=\u001b[36mTrue\u001b[39;49;00m).T.values[:, :, np.newaxis]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.n_signals = \u001b[36mself\u001b[39;49;00m.signals.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "\r\n",
      "        \u001b[37m# Define the action space as the portfolio weights where wn are [0, 1] for each asset not including cash\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.action_space = gym.spaces.Box(low=\u001b[34m0\u001b[39;49;00m, high=\u001b[34m1\u001b[39;49;00m, shape=(\u001b[36mself\u001b[39;49;00m.n_tickers,), dtype=np.float32)\r\n",
      "\r\n",
      "        \u001b[37m# Define the observation space, which are the signals\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.observation_space = gym.spaces.Box(low=-\u001b[34m1\u001b[39;49;00m, high=\u001b[34m1\u001b[39;49;00m, shape=(\u001b[36mself\u001b[39;49;00m.n_signals, \u001b[36mself\u001b[39;49;00m.window_length, \u001b[34m1\u001b[39;49;00m), dtype=np.float32)\r\n",
      "\r\n",
      "    \u001b[37m# -----------------------------------------------------------------------------------\u001b[39;49;00m\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mstep\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, action):\r\n",
      "        \u001b[33m\"\"\"Step the environment.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        See https://gym.openai.com/docs/#observations for detailed description of return values.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Args:\u001b[39;49;00m\r\n",
      "\u001b[33m            action (np.array):  The desired portfolio weights [w0...].\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m            np.array:  [n_signals, window_length, 1] The observation of the environment (state)\u001b[39;49;00m\r\n",
      "\u001b[33m            float:  The reward received from the previous action.\u001b[39;49;00m\r\n",
      "\u001b[33m            bool:  Indicates if the simulation is complete.\u001b[39;49;00m\r\n",
      "\u001b[33m            dict:  Debugging information.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.step_number += \u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# Force the new weights (w1) to (0.0, 1.0) and sum weights = 1, note 1st weight is cash.\u001b[39;49;00m\r\n",
      "        w1 = np.clip(action, a_min=\u001b[34m0\u001b[39;49;00m, a_max=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        w1 = np.insert(w1, \u001b[34m0\u001b[39;49;00m, np.clip(\u001b[34m1\u001b[39;49;00m - w1.sum(), a_min=\u001b[34m0\u001b[39;49;00m, a_max=\u001b[34m1\u001b[39;49;00m))\r\n",
      "        w1 = w1 / w1.sum()\r\n",
      "\r\n",
      "        \u001b[37m# Calculate the reward; Numbered equations are from https://arxiv.org/abs/1706.10059\u001b[39;49;00m\r\n",
      "        y1 = \u001b[36mself\u001b[39;49;00m.gain[\u001b[36mself\u001b[39;49;00m.step_number]\r\n",
      "        w0 = \u001b[36mself\u001b[39;49;00m.weights\r\n",
      "        p0 = \u001b[36mself\u001b[39;49;00m.portfolio_value\r\n",
      "        dw1 = (y1 * w0) / (np.dot(y1, w0) + EPS)            \u001b[37m# (eq7) weights evolve into\u001b[39;49;00m\r\n",
      "        mu1 = \u001b[36mself\u001b[39;49;00m.trading_cost * (np.abs(dw1 - w1)).sum()  \u001b[37m# (eq16) cost to change portfolio\u001b[39;49;00m\r\n",
      "        p1 = p0 * (\u001b[34m1\u001b[39;49;00m - mu1) * np.dot(y1, w1)                \u001b[37m# (eq11) final portfolio value\u001b[39;49;00m\r\n",
      "        p1 = np.clip(p1, \u001b[34m0\u001b[39;49;00m, np.inf)                         \u001b[37m# Limit portfolio to zero (busted)\u001b[39;49;00m\r\n",
      "        rho1 = p1 / p0 - \u001b[34m1\u001b[39;49;00m                                  \u001b[37m# rate of returns\u001b[39;49;00m\r\n",
      "        r1 = np.log((p1 + EPS) / (p0 + EPS))                \u001b[37m# log rate of return\u001b[39;49;00m\r\n",
      "        reward = \u001b[34m1000\u001b[39;49;00m * r1                                  \u001b[37m# (eq22) normalized logarithmic accumulated return\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# Save weights and portfolio value for next iteration\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.weights = w1\r\n",
      "        \u001b[36mself\u001b[39;49;00m.portfolio_value = p1\r\n",
      "\r\n",
      "        \u001b[37m# Observe the new environment (state)\u001b[39;49;00m\r\n",
      "        t = \u001b[36mself\u001b[39;49;00m.start_date_index + \u001b[36mself\u001b[39;49;00m.step_number\r\n",
      "        t0 = t - \u001b[36mself\u001b[39;49;00m.window_length + \u001b[34m1\u001b[39;49;00m\r\n",
      "        observation = \u001b[36mself\u001b[39;49;00m.signals[:, t0:t+\u001b[34m1\u001b[39;49;00m, :]\r\n",
      "\r\n",
      "        \u001b[37m# Save some information for debugging and plotting at the end\u001b[39;49;00m\r\n",
      "        r = y1.mean()\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.step_number == \u001b[34m1\u001b[39;49;00m:\r\n",
      "            market_value = r\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            market_value = \u001b[36mself\u001b[39;49;00m.info_list[-\u001b[34m1\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mmarket_value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] * r \r\n",
      "        info = {\u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: reward, \u001b[33m\"\u001b[39;49;00m\u001b[33mlog_return\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: r1, \u001b[33m\"\u001b[39;49;00m\u001b[33mportfolio_value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: p1, \u001b[33m\"\u001b[39;49;00m\u001b[33mreturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: r, \u001b[33m\"\u001b[39;49;00m\u001b[33mrate_of_return\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: rho1,\r\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mweights_mean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: w1.mean(), \u001b[33m\"\u001b[39;49;00m\u001b[33mweights_std\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: w1.std(), \u001b[33m\"\u001b[39;49;00m\u001b[33mcost\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: mu1, \u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mself\u001b[39;49;00m.dates[\u001b[36mself\u001b[39;49;00m.step_number],\r\n",
      "                \u001b[33m'\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mself\u001b[39;49;00m.step_number, \u001b[33m\"\u001b[39;49;00m\u001b[33mmarket_value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: market_value}\r\n",
      "        \u001b[36mself\u001b[39;49;00m.info_list.append(info)\r\n",
      "\r\n",
      "        \u001b[37m# Check if finished and write to file\u001b[39;49;00m\r\n",
      "        done = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.step_number >= \u001b[36mself\u001b[39;49;00m.steps) \u001b[35mor\u001b[39;49;00m (p1 <= \u001b[34m0\u001b[39;49;00m):\r\n",
      "            done = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "            pd.DataFrame(\u001b[36mself\u001b[39;49;00m.info_list).sort_values(by=[\u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]).to_csv(\u001b[36mself\u001b[39;49;00m.csv)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m observation, reward, done, info\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mreset\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"Reset the environment to the initial state.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Ref:\u001b[39;49;00m\r\n",
      "\u001b[33m        https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-rl-environments.html#sagemaker-rl-environments-gym\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.info_list = \u001b[36mlist\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.weights = np.insert(np.zeros(\u001b[36mself\u001b[39;49;00m.n_tickers), \u001b[34m0\u001b[39;49;00m, \u001b[34m1.0\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.portfolio_value = \u001b[34m1.0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.step_number = \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_date_index = \u001b[36mmax\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.start_date_index, \u001b[36mself\u001b[39;49;00m.window_length - \u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_date_index = \u001b[36mmin\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.start_date_index, \u001b[36mself\u001b[39;49;00m.n_dates - \u001b[36mself\u001b[39;49;00m.steps)\r\n",
      "\r\n",
      "        t = \u001b[36mself\u001b[39;49;00m.start_date_index + \u001b[36mself\u001b[39;49;00m.step_number\r\n",
      "        t0 = t - \u001b[36mself\u001b[39;49;00m.window_length + \u001b[34m1\u001b[39;49;00m\r\n",
      "        observation = \u001b[36mself\u001b[39;49;00m.signals[:, t0:t+\u001b[34m1\u001b[39;49;00m, :]\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m observation\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/portfolio_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Roles and permissions\n",
    "\n",
    "To get started, we'll import the Python libraries we need, set up the environment with a few prerequisites for permissions and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "from src.misc import get_execution_role, wait_for_s3_object\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "sys.path.append(\"common\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup S3 buckets\n",
    "\n",
    "Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-us-east-1-031118886020/\n"
     ]
    }
   ],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "Get the execution IAM role that allows this notebook to connect to the training and evaluation instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IAM role arn: arn:aws:iam::031118886020:role/sagemaker\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "print(\"Using IAM role arn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the RL model using the Python SDK Script mode\n",
    "\n",
    "1. Specify the source directory where the environment, presets and training code is uploaded, `src`.\n",
    "2. Specify the `entry_point` as the training code,  \n",
    "3. Specify the choice of RL `toolkit` and framework. This automatically resolves to the ECR path for the RL Container. \n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name. \n",
    "5. Specify the hyperparameters for the RL agent algorithm. The `RLCOACH_PRESET` can be used to specify the RL agent algorithm you want to use. \n",
    "6. [Optional] Choose the metrics that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks. The metrics are defined using regular expression matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price reference\n",
    "- ml.m4.4xlarge:  $414 s = 0.115 hr \\rightarrow \\$0.13 = 0.115 hr \\cdot 1.12 \\frac{\\$}{hr} $ \n",
    "- ml.p2.xlarge:   $713s = 0.198 hr \\rightarrow \\$0.25 = 0.198 hr \\cdot 1.26 \\frac{\\$}{hr}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-28 05:41:03 Starting - Starting the training job...\n",
      "2020-03-28 05:41:05 Starting - Launching requested ML instances.........\n",
      "2020-03-28 05:42:40 Starting - Preparing the instances for training.........\n",
      "2020-03-28 05:44:31 Downloading - Downloading input data...\n",
      "2020-03-28 05:44:53 Training - Downloading the training image.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-28 05:45:40,788 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-03-28 05:45:40,815 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_MODEL_DIR': '/opt/ml/model', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_estimator\":\"RLEstimator\"}', 'SM_HOSTS': '[\"algo-1\"]', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_USER_ARGS': '[\"--RLCOACH_PRESET\",\"preset-portfolio-management-clippedppo\",\"--rl.agent_params.algorithm.discount\",\"0.9\",\"--rl.evaluation_steps:EnvironmentEpisodes\",\"5\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_MODULE_NAME': 'train-coach', 'SM_HP_RL.EVALUATION_STEPS:ENVIRONMENTEPISODES': '5', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"drl-portfolio-optimization-2020-03-28-05-41-02-334\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-28-05-41-02-334/source/sourcedir.tar.gz\",\"module_name\":\"train-coach\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-coach.py\"}', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_CHANNELS': '[]', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_LOG_LEVEL': '20', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_USER_ENTRY_POINT': 'train-coach.py', 'SM_NUM_CPUS': '4', 'SM_HP_RLCOACH_PRESET': 'preset-portfolio-management-clippedppo', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-28-05-41-02-334/source/sourcedir.tar.gz', 'SM_HPS': '{\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5}', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_NUM_GPUS': '1', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CURRENT_HOST': 'algo-1', 'SM_HP_RL.AGENT_PARAMS.ALGORITHM.DISCOUNT': '0.9'}\u001b[0m\n",
      "\u001b[34m2020-03-28 05:45:41,001 sagemaker-containers INFO     Module train-coach does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-28 05:45:41,001 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-28 05:45:41,001 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-28 05:45:41,001 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train-coach\n",
      "  Running setup.py bdist_wheel for train-coach: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train-coach: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-665l7k04/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train-coach\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train-coach\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-coach-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-03-28 05:45:43,377 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator\": \"RLEstimator\"\n",
      "    },\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hyperparameters\": {\n",
      "        \"RLCOACH_PRESET\": \"preset-portfolio-management-clippedppo\",\n",
      "        \"rl.agent_params.algorithm.discount\": 0.9,\n",
      "        \"rl.evaluation_steps:EnvironmentEpisodes\": 5\n",
      "    },\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-28-05-41-02-334/source/sourcedir.tar.gz\",\n",
      "    \"input_data_config\": {},\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"user_entry_point\": \"train-coach.py\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"module_name\": \"train-coach\",\n",
      "    \"log_level\": 20,\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"job_name\": \"drl-portfolio-optimization-2020-03-28-05-41-02-334\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"model_dir\": \"/opt/ml/model\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator\":\"RLEstimator\"}\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-coach\u001b[0m\n",
      "\u001b[34mSM_HP_RL.EVALUATION_STEPS:ENVIRONMENTEPISODES=5\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"drl-portfolio-optimization-2020-03-28-05-41-02-334\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-28-05-41-02-334/source/sourcedir.tar.gz\",\"module_name\":\"train-coach\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-coach.py\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--RLCOACH_PRESET\",\"preset-portfolio-management-clippedppo\",\"--rl.agent_params.algorithm.discount\",\"0.9\",\"--rl.evaluation_steps:EnvironmentEpisodes\",\"5\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-coach.py\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_HP_RLCOACH_PRESET=preset-portfolio-management-clippedppo\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-28-05-41-02-334/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_HPS={\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_HP_RL.AGENT_PARAMS.ALGORITHM.DISCOUNT=0.9\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train-coach --RLCOACH_PRESET preset-portfolio-management-clippedppo --rl.agent_params.algorithm.discount 0.9 --rl.evaluation_steps:EnvironmentEpisodes 5\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#033[93mWarning: failed to import the following packages - tensorflow#033[0m\u001b[0m\n",
      "\u001b[34mApplying RL hyperparameter rl.agent_params.algorithm.discount=0.9\u001b[0m\n",
      "\u001b[34mApplying RL hyperparameter rl.evaluation_steps:EnvironmentEpisodes=5\u001b[0m\n",
      "\u001b[34mLoading preset preset-portfolio-management-clippedppo from /opt/ml/code\u001b[0m\n",
      "\u001b[34m## Creating graph - name: BasicRLGraphManager\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\u001b[0m\n",
      "\u001b[34m## Creating agent - name: agent\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m[05:45:50] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:109: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m## Starting to improve simple_rl_graph task index 0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-03-28 05:45:39 Training - Training image download completed. Training in progress.\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=1, Total reward=-613.58, Steps=506, Training iteration=0\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=2, Total reward=-576.92, Steps=1012, Training iteration=0\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=3, Total reward=-714.63, Steps=1518, Training iteration=0\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=4, Total reward=-982.21, Steps=2024, Training iteration=0\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-354.38, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-576.89, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-513.94, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-386.02, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-305.74, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -427.39\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/0_Step-2049.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=5, Total reward=-909.72, Steps=2554, Training iteration=0\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/rl_coach/architectures/mxnet_components/heads/head.py:95: UserWarning: Parameter clippedppolosscontinuous0_kl_coefficient is not used by any computation. Is this intended?\n",
      "  outputs = super(HeadLoss, self).forward(*args)\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013118846341967583, KL divergence=[0.], Entropy=[-0.14191103], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.032766472548246384, KL divergence=[0.], Entropy=[-0.14190392], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.05768756940960884, KL divergence=[0.], Entropy=[-0.14197683], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.07789019495248795, KL divergence=[0.], Entropy=[-0.1419561], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.08843903243541718, KL divergence=[0.], Entropy=[-0.14195855], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0948711559176445, KL divergence=[0.], Entropy=[-0.14195591], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.09875030070543289, KL divergence=[0.], Entropy=[-0.1419558], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.10140868276357651, KL divergence=[0.], Entropy=[-0.14194313], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.10390128195285797, KL divergence=[0.], Entropy=[-0.14194408], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.10556027293205261, KL divergence=[0.], Entropy=[-0.14193079], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/1_Step-2554.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=6, Total reward=-768.02, Steps=3060, Training iteration=1\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=7, Total reward=-1087.19, Steps=3566, Training iteration=1\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=8, Total reward=-802.25, Steps=4072, Training iteration=1\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=447.84, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=482.27, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=474.99, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=433.83, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=692.37, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 506.26\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/2_Step-4510.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=9, Total reward=-1051.92, Steps=4602, Training iteration=1\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02525753155350685, KL divergence=[0.], Entropy=[-0.14190732], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.06166961044073105, KL divergence=[0.], Entropy=[-0.14179009], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.06641288846731186, KL divergence=[0.], Entropy=[-0.14171775], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.07079606503248215, KL divergence=[0.], Entropy=[-0.14166422], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.06894230842590332, KL divergence=[0.], Entropy=[-0.14163649], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.06882937997579575, KL divergence=[0.], Entropy=[-0.14161357], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0739254429936409, KL divergence=[0.], Entropy=[-0.1415952], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.07386014610528946, KL divergence=[0.], Entropy=[-0.14156534], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.07689061760902405, KL divergence=[0.], Entropy=[-0.14153948], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.07748755812644958, KL divergence=[0.], Entropy=[-0.14153446], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/3_Step-4602.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=10, Total reward=-1304.7, Steps=5108, Training iteration=2\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=11, Total reward=-1048.77, Steps=5614, Training iteration=2\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=12, Total reward=-1222.7, Steps=6120, Training iteration=2\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=1033.16, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=888.39, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=999.2, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=975.8, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=1032.08, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 985.73\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/4_Step-6567.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=13, Total reward=-1278.66, Steps=6650, Training iteration=2\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012520763091742992, KL divergence=[0.], Entropy=[-0.14148298], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03159491717815399, KL divergence=[0.], Entropy=[-0.14135014], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0348406657576561, KL divergence=[0.], Entropy=[-0.14124963], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.041442062705755234, KL divergence=[0.], Entropy=[-0.14118959], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.04482588917016983, KL divergence=[0.], Entropy=[-0.14112489], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.04317716509103775, KL divergence=[0.], Entropy=[-0.1410798], training epoch=5, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=-0.05255842208862305, KL divergence=[0.], Entropy=[-0.14102091], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.046010810881853104, KL divergence=[0.], Entropy=[-0.14097132], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.05191521719098091, KL divergence=[0.], Entropy=[-0.14092134], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.053970400243997574, KL divergence=[0.], Entropy=[-0.14088461], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/5_Step-6650.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=14, Total reward=-1058.76, Steps=7156, Training iteration=3\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=15, Total reward=-1616.81, Steps=7662, Training iteration=3\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=16, Total reward=-1990.04, Steps=8168, Training iteration=3\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=1261.34, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=1231.54, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=1238.0, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=1185.79, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=1148.46, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1213.03\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/6_Step-8574.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=17, Total reward=-1407.24, Steps=8698, Training iteration=3\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.001162422471679747, KL divergence=[0.], Entropy=[-0.14083293], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01506350003182888, KL divergence=[0.], Entropy=[-0.14075306], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03260720893740654, KL divergence=[0.], Entropy=[-0.14068286], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03445019945502281, KL divergence=[0.], Entropy=[-0.14061993], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03822682425379753, KL divergence=[0.], Entropy=[-0.14055476], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03950728103518486, KL divergence=[0.], Entropy=[-0.14049956], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03803829103708267, KL divergence=[0.], Entropy=[-0.14045703], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.036135219037532806, KL divergence=[0.], Entropy=[-0.14040403], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.04167376458644867, KL divergence=[0.], Entropy=[-0.14036456], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.032355524599552155, KL divergence=[0.], Entropy=[-0.14034007], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/7_Step-8698.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=18, Total reward=-1310.46, Steps=9204, Training iteration=4\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=19, Total reward=-1577.35, Steps=9710, Training iteration=4\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=20, Total reward=-1458.31, Steps=10216, Training iteration=4\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=1272.42, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=1264.44, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=1226.69, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=1306.46, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=1157.21, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1245.45\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/8_Step-10615.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=21, Total reward=-999.15, Steps=10746, Training iteration=4\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006190065760165453, KL divergence=[0.], Entropy=[-0.14031094], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011076342314481735, KL divergence=[0.], Entropy=[-0.14031392], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015708675608038902, KL divergence=[0.], Entropy=[-0.14032543], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02381219156086445, KL divergence=[0.], Entropy=[-0.14032903], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02132369764149189, KL divergence=[0.], Entropy=[-0.14034414], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01906505413353443, KL divergence=[0.], Entropy=[-0.14035712], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02089407481253147, KL divergence=[0.], Entropy=[-0.14036915], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.027498524636030197, KL divergence=[0.], Entropy=[-0.14038457], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.019789239391684532, KL divergence=[0.], Entropy=[-0.14040086], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.024600272998213768, KL divergence=[0.], Entropy=[-0.1404167], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/9_Step-10746.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=22, Total reward=-1463.03, Steps=11252, Training iteration=5\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=23, Total reward=-1488.37, Steps=11758, Training iteration=5\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=24, Total reward=-1269.2, Steps=12264, Training iteration=5\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=1320.12, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=1382.68, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=1325.82, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=1375.13, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=1335.08, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1347.77\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/10_Step-12703.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=25, Total reward=-1337.44, Steps=12794, Training iteration=5\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0017771375132724643, KL divergence=[0.], Entropy=[-0.14042309], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011069457046687603, KL divergence=[0.], Entropy=[-0.14040552], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013602078892290592, KL divergence=[0.], Entropy=[-0.1403797], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0191220473498106, KL divergence=[0.], Entropy=[-0.14035536], training epoch=3, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=-0.016963226720690727, KL divergence=[0.], Entropy=[-0.14033046], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01631525531411171, KL divergence=[0.], Entropy=[-0.14031914], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017911110073328018, KL divergence=[0.], Entropy=[-0.14029986], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021236833184957504, KL divergence=[0.], Entropy=[-0.14028023], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02868950553238392, KL divergence=[0.], Entropy=[-0.14027402], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021710939705371857, KL divergence=[0.], Entropy=[-0.14025992], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/11_Step-12794.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=26, Total reward=-1708.81, Steps=13300, Training iteration=6\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=27, Total reward=-1028.26, Steps=13806, Training iteration=6\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=28, Total reward=-1378.79, Steps=14312, Training iteration=6\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=1373.0, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=1285.84, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=1384.88, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=1385.89, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=1308.75, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1347.67\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/12_Step-14704.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=29, Total reward=-1644.19, Steps=14842, Training iteration=6\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002837131964042783, KL divergence=[0.], Entropy=[-0.14025609], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009046104736626148, KL divergence=[0.], Entropy=[-0.14023429], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017511475831270218, KL divergence=[0.], Entropy=[-0.14020325], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018001234158873558, KL divergence=[0.], Entropy=[-0.14018364], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01939738355576992, KL divergence=[0.], Entropy=[-0.1401762], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02069414034485817, KL divergence=[0.], Entropy=[-0.14016952], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023952141404151917, KL divergence=[0.], Entropy=[-0.14015569], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01833813637495041, KL divergence=[0.], Entropy=[-0.14015886], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02458876185119152, KL divergence=[0.], Entropy=[-0.14015852], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.024371199309825897, KL divergence=[0.], Entropy=[-0.14013965], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/13_Step-14842.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=30, Total reward=-1282.94, Steps=15348, Training iteration=7\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=31, Total reward=-1270.36, Steps=15854, Training iteration=7\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=32, Total reward=-1281.78, Steps=16360, Training iteration=7\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=1356.57, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=1339.77, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=1317.08, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=1373.39, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=1364.68, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1350.3\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/14_Step-16759.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=33, Total reward=-1631.53, Steps=16890, Training iteration=7\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.005173274781554937, KL divergence=[0.], Entropy=[-0.14013179], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004678492899984121, KL divergence=[0.], Entropy=[-0.1401057], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006649456452578306, KL divergence=[0.], Entropy=[-0.14007795], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017211196944117546, KL divergence=[0.], Entropy=[-0.14005125], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013952471315860748, KL divergence=[0.], Entropy=[-0.14004593], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018382543697953224, KL divergence=[0.], Entropy=[-0.1400409], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014084923081099987, KL divergence=[0.], Entropy=[-0.14002784], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018828174099326134, KL divergence=[0.], Entropy=[-0.1400197], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022939171642065048, KL divergence=[0.], Entropy=[-0.14000371], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.024118542671203613, KL divergence=[0.], Entropy=[-0.14000665], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/15_Step-16890.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=34, Total reward=-1579.71, Steps=17396, Training iteration=8\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=35, Total reward=-1576.08, Steps=17902, Training iteration=8\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=36, Total reward=-1172.41, Steps=18408, Training iteration=8\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=1477.11, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=1371.23, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=1367.67, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=1365.55, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=1435.37, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1403.39\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/16_Step-18843.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=37, Total reward=-1418.69, Steps=18938, Training iteration=8\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0010931415017694235, KL divergence=[0.], Entropy=[-0.1399991], training epoch=0, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=-0.009861359372735023, KL divergence=[0.], Entropy=[-0.14001647], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008444057777523994, KL divergence=[0.], Entropy=[-0.14003535], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022927802056074142, KL divergence=[0.], Entropy=[-0.14004968], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01876961812376976, KL divergence=[0.], Entropy=[-0.14006394], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020676493644714355, KL divergence=[0.], Entropy=[-0.1400851], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023801881819963455, KL divergence=[0.], Entropy=[-0.14010514], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022695180028676987, KL divergence=[0.], Entropy=[-0.140126], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023493018001317978, KL divergence=[0.], Entropy=[-0.14014854], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02297162637114525, KL divergence=[0.], Entropy=[-0.14017257], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/17_Step-18938.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=38, Total reward=-1154.5, Steps=19444, Training iteration=9\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=39, Total reward=-1307.52, Steps=19950, Training iteration=9\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=40, Total reward=-1522.95, Steps=20456, Training iteration=9\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=1494.83, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=1580.75, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=1454.35, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=1382.95, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=1527.99, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1488.18\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/18_Step-20887.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=41, Total reward=-1409.82, Steps=20986, Training iteration=9\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003225949825718999, KL divergence=[0.], Entropy=[-0.14019611], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009697506204247475, KL divergence=[0.], Entropy=[-0.14022255], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0067239548079669476, KL divergence=[0.], Entropy=[-0.14025018], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015077675692737103, KL divergence=[0.], Entropy=[-0.14028692], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017421577125787735, KL divergence=[0.], Entropy=[-0.14031395], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02461395412683487, KL divergence=[0.], Entropy=[-0.1403324], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021299080923199654, KL divergence=[0.], Entropy=[-0.14035594], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02104620821774006, KL divergence=[0.], Entropy=[-0.1403852], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02022627927362919, KL divergence=[0.], Entropy=[-0.14039259], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018507597967982292, KL divergence=[0.], Entropy=[-0.14040916], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/19_Step-20986.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=42, Total reward=-1234.36, Steps=21492, Training iteration=10\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=43, Total reward=-1157.72, Steps=21998, Training iteration=10\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=44, Total reward=-1048.44, Steps=22504, Training iteration=10\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=1579.42, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=1716.98, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=1603.28, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=1646.1, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=1678.38, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1644.83\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/20_Step-22929.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=45, Total reward=-1627.35, Steps=23034, Training iteration=10\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0008385147666558623, KL divergence=[0.], Entropy=[-0.14042003], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0038902342785149813, KL divergence=[0.], Entropy=[-0.1404284], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013647148385643959, KL divergence=[0.], Entropy=[-0.14043154], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009216764941811562, KL divergence=[0.], Entropy=[-0.14043571], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016158197075128555, KL divergence=[0.], Entropy=[-0.14045031], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018745148554444313, KL divergence=[0.], Entropy=[-0.14046107], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015487981028854847, KL divergence=[0.], Entropy=[-0.1404626], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01825009658932686, KL divergence=[0.], Entropy=[-0.1404789], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020236315205693245, KL divergence=[0.], Entropy=[-0.14048277], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018471451476216316, KL divergence=[0.], Entropy=[-0.14048506], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/21_Step-23034.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=46, Total reward=-1526.88, Steps=23540, Training iteration=11\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=47, Total reward=-914.39, Steps=24046, Training iteration=11\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=48, Total reward=-955.91, Steps=24552, Training iteration=11\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=1748.97, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=1815.31, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=1638.34, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=1729.03, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=1771.0, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1740.53\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/22_Step-25032.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=49, Total reward=-1776.17, Steps=25082, Training iteration=11\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0014089451869949698, KL divergence=[0.], Entropy=[-0.1404935], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003477459540590644, KL divergence=[0.], Entropy=[-0.14047585], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008161205798387527, KL divergence=[0.], Entropy=[-0.14047168], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011333165690302849, KL divergence=[0.], Entropy=[-0.14047232], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017716342583298683, KL divergence=[0.], Entropy=[-0.1404694], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015518497675657272, KL divergence=[0.], Entropy=[-0.1404615], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02110285498201847, KL divergence=[0.], Entropy=[-0.1404496], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02534724771976471, KL divergence=[0.], Entropy=[-0.14044242], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01751190423965454, KL divergence=[0.], Entropy=[-0.14044802], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.03310677409172058, KL divergence=[0.], Entropy=[-0.14044607], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/23_Step-25082.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=50, Total reward=-1448.36, Steps=25588, Training iteration=12\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=51, Total reward=-1371.13, Steps=26094, Training iteration=12\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=52, Total reward=-916.55, Steps=26600, Training iteration=12\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=1827.76, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=1824.23, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=1836.54, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=1742.86, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=1823.37, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1810.95\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/24_Step-27092.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=53, Total reward=-1238.19, Steps=27130, Training iteration=12\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00010162591934204102, KL divergence=[0.], Entropy=[-0.14042409], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009340458549559116, KL divergence=[0.], Entropy=[-0.14043172], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011704220436513424, KL divergence=[0.], Entropy=[-0.1404395], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01685119792819023, KL divergence=[0.], Entropy=[-0.14044558], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017499590292572975, KL divergence=[0.], Entropy=[-0.14044638], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015390700660645962, KL divergence=[0.], Entropy=[-0.14044857], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020037148147821426, KL divergence=[0.], Entropy=[-0.14045179], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023752771317958832, KL divergence=[0.], Entropy=[-0.14046267], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.019870318472385406, KL divergence=[0.], Entropy=[-0.14046511], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023507017642259598, KL divergence=[0.], Entropy=[-0.14047767], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/25_Step-27130.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=54, Total reward=-872.06, Steps=27636, Training iteration=13\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=55, Total reward=-996.69, Steps=28142, Training iteration=13\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=56, Total reward=-1624.27, Steps=28648, Training iteration=13\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=1848.04, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=1934.92, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=1889.98, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=1880.82, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=1846.71, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1880.1\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/26_Step-29092.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=57, Total reward=-1494.4, Steps=29178, Training iteration=13\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00038787242374382913, KL divergence=[0.], Entropy=[-0.14048478], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013916123658418655, KL divergence=[0.], Entropy=[-0.1405064], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010572833940386772, KL divergence=[0.], Entropy=[-0.14054139], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011934005655348301, KL divergence=[0.], Entropy=[-0.14056048], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011700751259922981, KL divergence=[0.], Entropy=[-0.14059237], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015459372662007809, KL divergence=[0.], Entropy=[-0.14062215], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018938088789582253, KL divergence=[0.], Entropy=[-0.14064795], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022418661043047905, KL divergence=[0.], Entropy=[-0.14067028], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018240878358483315, KL divergence=[0.], Entropy=[-0.14070006], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.026137586683034897, KL divergence=[0.], Entropy=[-0.14071816], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/27_Step-29178.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=58, Total reward=-965.59, Steps=29684, Training iteration=14\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=59, Total reward=-1213.54, Steps=30190, Training iteration=14\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=60, Total reward=-1051.48, Steps=30696, Training iteration=14\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=1879.89, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=1930.72, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=1946.45, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=1965.68, Steps=30720, Training iteration=14\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=1994.84, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1943.52\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/28_Step-31131.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=61, Total reward=-1044.88, Steps=31226, Training iteration=14\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.005044369027018547, KL divergence=[0.], Entropy=[-0.14073358], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0068208700977265835, KL divergence=[0.], Entropy=[-0.14074576], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005810163449496031, KL divergence=[0.], Entropy=[-0.14075936], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014986849389970303, KL divergence=[0.], Entropy=[-0.14077239], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013389911502599716, KL divergence=[0.], Entropy=[-0.14078085], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017201265320181847, KL divergence=[0.], Entropy=[-0.14080407], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02213154546916485, KL divergence=[0.], Entropy=[-0.14082393], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.018265483900904655, KL divergence=[0.], Entropy=[-0.14083251], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022695766761898994, KL divergence=[0.], Entropy=[-0.1408476], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021550143137574196, KL divergence=[0.], Entropy=[-0.14086547], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/29_Step-31226.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=62, Total reward=-1126.49, Steps=31732, Training iteration=15\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=63, Total reward=-1219.64, Steps=32238, Training iteration=15\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=64, Total reward=-1294.82, Steps=32744, Training iteration=15\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=1945.92, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=1923.78, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=1902.64, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=1853.43, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=64, Total reward=1959.08, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1916.97\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/30_Step-33192.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=65, Total reward=-1204.14, Steps=33274, Training iteration=15\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.000795005529653281, KL divergence=[0.], Entropy=[-0.14085503], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003970325458794832, KL divergence=[0.], Entropy=[-0.14082834], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008341499604284763, KL divergence=[0.], Entropy=[-0.14080182], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01051266584545374, KL divergence=[0.], Entropy=[-0.14078635], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017684169113636017, KL divergence=[0.], Entropy=[-0.14076383], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012301553040742874, KL divergence=[0.], Entropy=[-0.14073935], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02074415236711502, KL divergence=[0.], Entropy=[-0.14072672], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017554113641381264, KL divergence=[0.], Entropy=[-0.14071833], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016435252502560616, KL divergence=[0.], Entropy=[-0.14069578], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.022866329178214073, KL divergence=[0.], Entropy=[-0.14067717], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/31_Step-33274.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=66, Total reward=-1276.88, Steps=33780, Training iteration=16\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=67, Total reward=-847.23, Steps=34286, Training iteration=16\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=68, Total reward=-793.34, Steps=34792, Training iteration=16\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=1837.8, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=1980.31, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=1989.86, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=1864.8, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=68, Total reward=1911.29, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1916.81\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/32_Step-35241.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=69, Total reward=-1110.36, Steps=35322, Training iteration=16\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005329381208866835, KL divergence=[0.], Entropy=[-0.14066797], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009211706928908825, KL divergence=[0.], Entropy=[-0.14065367], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007553307339549065, KL divergence=[0.], Entropy=[-0.14064609], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005772850941866636, KL divergence=[0.], Entropy=[-0.14063802], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008806122466921806, KL divergence=[0.], Entropy=[-0.14063005], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011351858265697956, KL divergence=[0.], Entropy=[-0.14061807], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01832142099738121, KL divergence=[0.], Entropy=[-0.14060068], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0172079186886549, KL divergence=[0.], Entropy=[-0.14059208], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016292987391352654, KL divergence=[0.], Entropy=[-0.1405868], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01783723384141922, KL divergence=[0.], Entropy=[-0.14057264], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/33_Step-35322.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=70, Total reward=-1042.87, Steps=35828, Training iteration=17\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=71, Total reward=-1186.27, Steps=36334, Training iteration=17\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=72, Total reward=-1397.45, Steps=36840, Training iteration=17\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=2008.14, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=1981.52, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=1893.08, Steps=36864, Training iteration=17\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=1921.44, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=72, Total reward=1945.31, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 1949.9\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/34_Step-37287.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=73, Total reward=-1002.58, Steps=37370, Training iteration=17\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0031100858468562365, KL divergence=[0.], Entropy=[-0.14058532], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009585578925907612, KL divergence=[0.], Entropy=[-0.14061365], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00948480423539877, KL divergence=[0.], Entropy=[-0.14065269], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012733472511172295, KL divergence=[0.], Entropy=[-0.14066926], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015328445471823215, KL divergence=[0.], Entropy=[-0.14069341], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02147832326591015, KL divergence=[0.], Entropy=[-0.1407047], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.020926583558321, KL divergence=[0.], Entropy=[-0.14070816], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01524087879806757, KL divergence=[0.], Entropy=[-0.14071797], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01606457121670246, KL divergence=[0.], Entropy=[-0.14071834], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016452064737677574, KL divergence=[0.], Entropy=[-0.14071007], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/35_Step-37370.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=74, Total reward=-763.07, Steps=37876, Training iteration=18\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=75, Total reward=-845.59, Steps=38382, Training iteration=18\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=76, Total reward=-700.67, Steps=38888, Training iteration=18\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=2074.56, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=1961.1, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=1977.46, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=1945.48, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=76, Total reward=2069.71, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2005.66\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/36_Step-39305.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=77, Total reward=-1506.63, Steps=39418, Training iteration=18\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00033139335573650897, KL divergence=[0.], Entropy=[-0.14071108], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008140016347169876, KL divergence=[0.], Entropy=[-0.14072461], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00669456971809268, KL divergence=[0.], Entropy=[-0.14074817], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014196626842021942, KL divergence=[0.], Entropy=[-0.14075585], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016387131065130234, KL divergence=[0.], Entropy=[-0.14077702], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015477992594242096, KL divergence=[0.], Entropy=[-0.14079723], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010877571068704128, KL divergence=[0.], Entropy=[-0.1408002], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016128361225128174, KL divergence=[0.], Entropy=[-0.14081398], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014306032098829746, KL divergence=[0.], Entropy=[-0.14082527], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.019002484157681465, KL divergence=[0.], Entropy=[-0.14083497], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/37_Step-39418.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=78, Total reward=-755.16, Steps=39924, Training iteration=19\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=79, Total reward=-638.06, Steps=40430, Training iteration=19\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=80, Total reward=-1078.82, Steps=40936, Training iteration=19\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=2089.7, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=2069.15, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=1975.2, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=2031.97, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=80, Total reward=1968.97, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2027.0\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/38_Step-41378.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=81, Total reward=-879.39, Steps=41466, Training iteration=19\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0033681883942335844, KL divergence=[0.], Entropy=[-0.14083567], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005560289137065411, KL divergence=[0.], Entropy=[-0.14081644], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0016103088855743408, KL divergence=[0.], Entropy=[-0.14080833], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00846134964376688, KL divergence=[0.], Entropy=[-0.14080986], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009024055674672127, KL divergence=[0.], Entropy=[-0.14080003], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016966622322797775, KL divergence=[0.], Entropy=[-0.14080268], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013001961633563042, KL divergence=[0.], Entropy=[-0.14078334], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016188563778996468, KL divergence=[0.], Entropy=[-0.14077444], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015943942591547966, KL divergence=[0.], Entropy=[-0.14076835], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013657924719154835, KL divergence=[0.], Entropy=[-0.1407618], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/39_Step-41466.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=82, Total reward=-785.31, Steps=41972, Training iteration=20\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=83, Total reward=-1507.38, Steps=42478, Training iteration=20\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=84, Total reward=-1317.19, Steps=42984, Training iteration=20\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=2094.7, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=2055.64, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=2091.14, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=2026.78, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=84, Total reward=2055.1, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2064.67\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/40_Step-43362.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=85, Total reward=-892.93, Steps=43514, Training iteration=20\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=0.0022749481722712517, KL divergence=[0.], Entropy=[-0.14075609], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002113525988534093, KL divergence=[0.], Entropy=[-0.14076172], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006883290596306324, KL divergence=[0.], Entropy=[-0.14076914], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009712322615087032, KL divergence=[0.], Entropy=[-0.14077666], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01625751703977585, KL divergence=[0.], Entropy=[-0.14078242], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014189667999744415, KL divergence=[0.], Entropy=[-0.1407797], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011757711879909039, KL divergence=[0.], Entropy=[-0.14077869], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016941510140895844, KL divergence=[0.], Entropy=[-0.14078283], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009650105610489845, KL divergence=[0.], Entropy=[-0.14079137], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016811126843094826, KL divergence=[0.], Entropy=[-0.14078824], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/41_Step-43514.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=86, Total reward=-1194.54, Steps=44020, Training iteration=21\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=87, Total reward=-977.03, Steps=44526, Training iteration=21\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=88, Total reward=-1431.36, Steps=45032, Training iteration=21\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=2149.62, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=2020.69, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=2038.58, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=2071.33, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=88, Total reward=2109.81, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2078.01\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/42_Step-45478.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=89, Total reward=-1379.14, Steps=45562, Training iteration=21\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-2.8051195840816945e-05, KL divergence=[0.], Entropy=[-0.14079224], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01224533747881651, KL divergence=[0.], Entropy=[-0.14080893], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008109231479465961, KL divergence=[0.], Entropy=[-0.14083011], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011983204632997513, KL divergence=[0.], Entropy=[-0.14083478], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014370905235409737, KL divergence=[0.], Entropy=[-0.14084555], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014557079412043095, KL divergence=[0.], Entropy=[-0.14085595], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014959895983338356, KL divergence=[0.], Entropy=[-0.14085636], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.021073095500469208, KL divergence=[0.], Entropy=[-0.14085753], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.023640427738428116, KL divergence=[0.], Entropy=[-0.1408529], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.017055047675967216, KL divergence=[0.], Entropy=[-0.14085345], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/43_Step-45562.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=90, Total reward=-1225.03, Steps=46068, Training iteration=22\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=91, Total reward=-763.62, Steps=46574, Training iteration=22\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=92, Total reward=-1017.9, Steps=47080, Training iteration=22\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=2135.4, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=2190.16, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=2132.8, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=2176.99, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=92, Total reward=2113.62, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2149.79\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/44_Step-47542.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=93, Total reward=-894.05, Steps=47610, Training iteration=22\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.001682835747487843, KL divergence=[0.], Entropy=[-0.14085326], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005131242331117392, KL divergence=[0.], Entropy=[-0.14085315], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009779266081750393, KL divergence=[0.], Entropy=[-0.14083278], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00874139741063118, KL divergence=[0.], Entropy=[-0.14081268], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006816472392529249, KL divergence=[0.], Entropy=[-0.1408034], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0127933444455266, KL divergence=[0.], Entropy=[-0.14078398], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011613202281296253, KL divergence=[0.], Entropy=[-0.1407753], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012473667971789837, KL divergence=[0.], Entropy=[-0.14079113], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01449735276401043, KL divergence=[0.], Entropy=[-0.14079481], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012513646855950356, KL divergence=[0.], Entropy=[-0.1407908], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/45_Step-47610.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=94, Total reward=-604.2, Steps=48116, Training iteration=23\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=95, Total reward=-1089.26, Steps=48622, Training iteration=23\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=96, Total reward=-907.6, Steps=49128, Training iteration=23\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=2252.06, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=2220.78, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=2272.72, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=2207.56, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=96, Total reward=2186.09, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2227.84\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/46_Step-49537.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=97, Total reward=-1167.79, Steps=49658, Training iteration=23\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPolicy training> Surrogate loss=-0.001464222208596766, KL divergence=[0.], Entropy=[-0.1408081], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003936357330530882, KL divergence=[0.], Entropy=[-0.14082545], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0064239539206027985, KL divergence=[0.], Entropy=[-0.14083599], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00826327409595251, KL divergence=[0.], Entropy=[-0.14084671], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012728201225399971, KL divergence=[0.], Entropy=[-0.14084302], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007882197387516499, KL divergence=[0.], Entropy=[-0.14085661], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010824225842952728, KL divergence=[0.], Entropy=[-0.14086497], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01314351987093687, KL divergence=[0.], Entropy=[-0.14086568], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.014148950576782227, KL divergence=[0.], Entropy=[-0.14086351], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.02204945869743824, KL divergence=[0.], Entropy=[-0.14086394], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/47_Step-49658.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=98, Total reward=-868.71, Steps=50164, Training iteration=24\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=99, Total reward=-1004.58, Steps=50670, Training iteration=24\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=100, Total reward=-938.37, Steps=51176, Training iteration=24\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=2339.29, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=2349.22, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=2330.0, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=2301.83, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=100, Total reward=2323.29, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2328.73\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/48_Step-51603.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=101, Total reward=-689.83, Steps=51706, Training iteration=24\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0036706868559122086, KL divergence=[0.], Entropy=[-0.14084561], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00435967743396759, KL divergence=[0.], Entropy=[-0.14083019], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006990407593548298, KL divergence=[0.], Entropy=[-0.14081101], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.016558360308408737, KL divergence=[0.], Entropy=[-0.14078213], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01082291267812252, KL divergence=[0.], Entropy=[-0.14078173], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013269605115056038, KL divergence=[0.], Entropy=[-0.14078423], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.013562004081904888, KL divergence=[0.], Entropy=[-0.14079684], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.011380497366189957, KL divergence=[0.], Entropy=[-0.14078389], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009941994212567806, KL divergence=[0.], Entropy=[-0.14077786], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.015349089168012142, KL divergence=[0.], Entropy=[-0.14077856], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/49_Step-51706.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=102, Total reward=-1076.51, Steps=52212, Training iteration=25\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=103, Total reward=-1141.5, Steps=52718, Training iteration=25\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=104, Total reward=-1190.72, Steps=53224, Training iteration=25\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=2280.79, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=2338.38, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=2326.96, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=2361.64, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=104, Total reward=2321.31, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2325.81\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/50_Step-53674.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=105, Total reward=-920.07, Steps=53754, Training iteration=25\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0024057819973677397, KL divergence=[0.], Entropy=[-0.14079116], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0029871261212974787, KL divergence=[0.], Entropy=[-0.1408192], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008674710988998413, KL divergence=[0.], Entropy=[-0.14084287], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.002601840766146779, KL divergence=[0.], Entropy=[-0.14084715], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007058767136186361, KL divergence=[0.], Entropy=[-0.14087085], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00733688659965992, KL divergence=[0.], Entropy=[-0.14087205], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009138609282672405, KL divergence=[0.], Entropy=[-0.14085576], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010212124325335026, KL divergence=[0.], Entropy=[-0.1408517], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.009451212361454964, KL divergence=[0.], Entropy=[-0.1408531], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012765568681061268, KL divergence=[0.], Entropy=[-0.1408669], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/51_Step-53754.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=106, Total reward=-1242.75, Steps=54260, Training iteration=26\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=107, Total reward=-1212.26, Steps=54766, Training iteration=26\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=108, Total reward=-841.13, Steps=55272, Training iteration=26\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=2364.77, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=2298.95, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=2304.58, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=2330.01, Steps=55296, Training iteration=26\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=108, Total reward=2424.81, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2344.62\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/52_Step-55725.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=109, Total reward=-657.6, Steps=55802, Training iteration=26\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.00748540461063385, KL divergence=[0.], Entropy=[-0.14087062], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0033608179073780775, KL divergence=[0.], Entropy=[-0.14086488], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005329885985702276, KL divergence=[0.], Entropy=[-0.14086385], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0019508054247125983, KL divergence=[0.], Entropy=[-0.14086421], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008379574865102768, KL divergence=[0.], Entropy=[-0.1408594], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.012764007784426212, KL divergence=[0.], Entropy=[-0.14085983], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003230802481994033, KL divergence=[0.], Entropy=[-0.1408598], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00863066129386425, KL divergence=[0.], Entropy=[-0.14085265], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008615286089479923, KL divergence=[0.], Entropy=[-0.14085914], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.01184391975402832, KL divergence=[0.], Entropy=[-0.14085898], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/53_Step-55802.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=110, Total reward=-372.1, Steps=56308, Training iteration=27\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=111, Total reward=-744.32, Steps=56814, Training iteration=27\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=112, Total reward=-1085.77, Steps=57320, Training iteration=27\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=2351.52, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=2425.09, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=2430.15, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=2459.77, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=112, Total reward=2370.42, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2407.39\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/54_Step-57779.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=113, Total reward=-604.63, Steps=57850, Training iteration=27\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0014168346533551812, KL divergence=[0.], Entropy=[-0.14085437], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0009589563123881817, KL divergence=[0.], Entropy=[-0.14084776], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0018897695699706674, KL divergence=[0.], Entropy=[-0.1408476], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.004995832685381174, KL divergence=[0.], Entropy=[-0.1408542], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003013460896909237, KL divergence=[0.], Entropy=[-0.1408529], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.00681390892714262, KL divergence=[0.], Entropy=[-0.14084893], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.010588559322059155, KL divergence=[0.], Entropy=[-0.14083761], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006536516826599836, KL divergence=[0.], Entropy=[-0.14085105], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006746190600097179, KL divergence=[0.], Entropy=[-0.1408531], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0059357574209570885, KL divergence=[0.], Entropy=[-0.14086334], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/55_Step-57850.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=114, Total reward=-861.82, Steps=58356, Training iteration=28\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=115, Total reward=-547.17, Steps=58862, Training iteration=28\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=116, Total reward=-948.54, Steps=59368, Training iteration=28\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=2462.98, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=2449.85, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=2329.26, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=2416.64, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=116, Total reward=2422.17, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2416.18\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/56_Step-59818.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=117, Total reward=-1004.8, Steps=59898, Training iteration=28\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=0.0004953799652867019, KL divergence=[0.], Entropy=[-0.14088029], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0016908618854358792, KL divergence=[0.], Entropy=[-0.14089783], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.003100387519225478, KL divergence=[0.], Entropy=[-0.14091718], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.006681737024337053, KL divergence=[0.], Entropy=[-0.14092393], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0014187596971169114, KL divergence=[0.], Entropy=[-0.1409202], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005696211475878954, KL divergence=[0.], Entropy=[-0.1409271], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.005601624492555857, KL divergence=[0.], Entropy=[-0.14092691], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.0015130097744986415, KL divergence=[0.], Entropy=[-0.14093485], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.007935554720461369, KL divergence=[0.], Entropy=[-0.14093333], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mPolicy training> Surrogate loss=-0.008277533575892448, KL divergence=[0.], Entropy=[-0.14092092], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[34mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/57_Step-59898.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=118, Total reward=-835.42, Steps=60404, Training iteration=29\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=119, Total reward=-718.34, Steps=60910, Training iteration=29\u001b[0m\n",
      "\u001b[34mTraining> Name=main_level/agent, Worker=0, Episode=120, Total reward=-704.54, Steps=61416, Training iteration=29\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=2513.91, Steps=61440, Training iteration=29\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=2500.27, Steps=61440, Training iteration=29\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=2495.69, Steps=61440, Training iteration=29\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=2498.44, Steps=61440, Training iteration=29\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=120, Total reward=2434.37, Steps=61440, Training iteration=29\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 2488.53\u001b[0m\n",
      "\u001b[34m2020-03-28 05:56:11,058 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-28 05:56:24 Uploading - Uploading generated training model\n",
      "2020-03-28 05:56:24 Completed - Training job completed\n",
      "Training seconds: 713\n",
      "Billable seconds: 713\n"
     ]
    }
   ],
   "source": [
    "job_name_prefix = 'drl-portfolio-optimization'\n",
    "estimator = RLEstimator(source_dir='src',\n",
    "                        entry_point=\"train-coach.py\",\n",
    "                        dependencies=[\"common/sagemaker_rl\"],\n",
    "                        toolkit=RLToolkit.COACH,\n",
    "                        toolkit_version='0.11.0',\n",
    "                        framework=RLFramework.MXNET,\n",
    "                        role=role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type=\"ml.m4.4xlarge\",\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        hyperparameters = {\n",
    "                          \"RLCOACH_PRESET\" : \"preset-portfolio-management-clippedppo\",\n",
    "                          \"rl.agent_params.algorithm.discount\": 0.9,\n",
    "                          \"rl.evaluation_steps:EnvironmentEpisodes\": 5\n",
    "                      }\n",
    "                    )\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer the training output and model checkpoints back to Sagemarker instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name: drl-portfolio-optimization-2020-03-28-05-41-02-334\n",
      "\n",
      "Waiting for s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-28-05-41-02-334/output/output.tar.gz...\n",
      "Downloading drl-portfolio-optimization-2020-03-28-05-41-02-334/output/output.tar.gz\n",
      "Copied output files to /tmp/drl-portfolio-optimization-2020-03-28-05-41-02-334\n"
     ]
    }
   ],
   "source": [
    "job_name=estimator._current_job_name\n",
    "output_tar_key = \"{}/output/output.tar.gz\".format(job_name)\n",
    "intermediate_folder_key = \"{}/output/intermediate/\".format(job_name)\n",
    "intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key)\n",
    "\n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "print(\"Job name: {}\\n\".format(job_name))\n",
    "\n",
    "wait_for_s3_object(s3_bucket, output_tar_key, tmp_dir)  \n",
    "\n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "os.system(\"aws s3 cp --recursive {} {}\".format(intermediate_url, tmp_dir))\n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "print(\"Copied output files to {}\".format(tmp_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot rate of learning\n",
    "\n",
    "We can view the rewards during training using the code below. This visualization helps us understand how the performance of the model represented as the reward has improved over time. For the consideration of training time, we restict the episodes number. If you see the final reward (average logarithmic cumulated return) is still below zero, try a larger training steps. The number of steps can be configured in the preset file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-28-05-41-02-334/output/intermediate/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv...\n",
      "Downloading drl-portfolio-optimization-2020-03-28-05-41-02-334/output/intermediate/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "csv_file_name = \"worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\"\n",
    "key = os.path.join(intermediate_folder_key, csv_file_name)\n",
    "wait_for_s3_object(s3_bucket, key, tmp_dir)\n",
    "\n",
    "csv_file = \"{}/{}\".format(tmp_dir, csv_file_name)\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.dropna(subset=['Evaluation Reward'])\n",
    "x_axis = 'Episode #'\n",
    "y_axis = 'Evaluation Reward'\n",
    "\n",
    "plt = df.plot(x=x_axis,y=y_axis, figsize=(12,5), legend=True, style='b-')\n",
    "plt.set_ylabel(y_axis);\n",
    "plt.set_xlabel(x_axis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the portfolio value\n",
    "\n",
    "We use result of the last evaluation phase as an example to visualize the portfolio value. The following figure demonstrates reward vs date. Sharpe ratio and maximum drawdown are also calculated to help readers understand the return of an investment compared to its risk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# same as in https://github.com/vermouth1992/drl-portfolio-management/blob/master/src/environment/portfolio.py\n",
    "def sharpe(returns, freq=30, rfr=0):\n",
    "    \"\"\" Given a set of returns, calculates naive (rfr=0) sharpe. \"\"\"\n",
    "    eps = np.finfo(np.float32).eps\n",
    "    return (np.sqrt(freq) * np.mean(returns - rfr + eps)) / np.std(returns - rfr + eps)\n",
    "\n",
    "\n",
    "def max_drawdown(returns):\n",
    "    \"\"\" Max drawdown. See https://www.investopedia.com/terms/m/maximum-drawdown-mdd.asp \"\"\"\n",
    "    eps = np.finfo(np.float32).eps\n",
    "    peak = returns.max()\n",
    "    trough = returns[np.argmax(returns):].min()\n",
    "    return (trough - peak) / (peak + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2c644c29b0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "info = info_dir + 'portfolio-management.csv'\n",
    "df_info = pd.read_csv(info)\n",
    "df_info['date'] = pd.to_datetime(df_info['date'], format='%Y-%m-%d')\n",
    "df_info.set_index('date', inplace=True)\n",
    "mdd = max_drawdown(df_info.rate_of_return + 1)\n",
    "sharpe_ratio = sharpe(df_info.rate_of_return)\n",
    "title = 'max_drawdown={: 2.2%} sharpe_ratio={: 2.4f}'.format(mdd, sharpe_ratio)\n",
    "df_info[[\"portfolio_value\", \"market_value\"]].plot(title=title, fig=plt.gcf(), rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the checkpointed models for evaluation\n",
    "\n",
    "Checkpointed data from the previously trained models will be passed on for evaluation / inference in the `checkpoint` channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 checkpoint file path: s3://sagemaker-us-east-1-031118886020/drl-portfolio-optimization-2020-03-28-05-17-27-714/checkpoint/\n",
      "CPU times: user 676 s, sys: 4.22 ms, total: 4.89 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "checkpoint_path = \"s3://{}/{}/checkpoint/\".format(s3_bucket, job_name)\n",
    "if not os.listdir(checkpoint_dir):\n",
    "    raise FileNotFoundError(\"Checkpoint files not found under the path\")\n",
    "os.system(\"aws s3 cp --recursive {} {}\".format(checkpoint_dir, checkpoint_path))\n",
    "print(\"S3 checkpoint file path: {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the evaluation step\n",
    "\n",
    "Use the checkpointed model to run the evaluation step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-28 05:31:57 Starting - Starting the training job...\n",
      "2020-03-28 05:31:58 Starting - Launching requested ML instances.........\n",
      "2020-03-28 05:33:38 Starting - Preparing the instances for training......\n",
      "2020-03-28 05:34:51 Downloading - Downloading input data\n",
      "2020-03-28 05:34:51 Training - Downloading the training image...\n",
      "2020-03-28 05:35:26 Uploading - Uploading generated training model\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:12,448 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:12,452 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:12,466 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CURRENT_HOST': 'algo-1', 'SM_HOSTS': '[\"algo-1\"]', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{\"checkpoint\":\"/opt/ml/input/data/checkpoint\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"evaluate_steps\":1462},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"checkpoint\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-rl-mxnet-2020-03-28-05-31-55-430\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-28-05-31-55-430/source/sourcedir.tar.gz\",\"module_name\":\"evaluate-coach\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"evaluate-coach.py\"}', 'SM_MODULE_NAME': 'evaluate-coach', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_estimator\":\"RLEstimator\"}', 'SM_HP_EVALUATE_STEPS': '1462', 'SM_HPS': '{\"evaluate_steps\":1462}', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-28-05-31-55-430/source/sourcedir.tar.gz', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"checkpoint\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_GPUS': '0', 'SM_USER_ENTRY_POINT': 'evaluate-coach.py', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_NUM_CPUS': '16', 'SM_LOG_LEVEL': '20', 'SM_USER_ARGS': '[\"--evaluate_steps\",\"1462\"]', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_CHANNELS': '[\"checkpoint\"]', 'SM_CHANNEL_CHECKPOINT': '/opt/ml/input/data/checkpoint', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main'}\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:12,736 sagemaker-containers INFO     Module evaluate-coach does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:12,736 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:12,736 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:12,736 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: evaluate-coach\n",
      "  Running setup.py bdist_wheel for evaluate-coach: started\n",
      "  Running setup.py bdist_wheel for evaluate-coach: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2pr54wgc/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built evaluate-coach\u001b[0m\n",
      "\u001b[34mInstalling collected packages: evaluate-coach\u001b[0m\n",
      "\u001b[34mSuccessfully installed evaluate-coach-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:15,012 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:15,027 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"module_name\": \"evaluate-coach\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ]\n",
      "    },\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"user_entry_point\": \"evaluate-coach.py\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"checkpoint\": \"/opt/ml/input/data/checkpoint\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-28-05-31-55-430/source/sourcedir.tar.gz\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator\": \"RLEstimator\"\n",
      "    },\n",
      "    \"hyperparameters\": {\n",
      "        \"evaluate_steps\": 1462\n",
      "    },\n",
      "    \"input_data_config\": {\n",
      "        \"checkpoint\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"TrainingInputMode\": \"File\"\n",
      "        }\n",
      "    },\n",
      "    \"log_level\": 20,\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"job_name\": \"sagemaker-rl-mxnet-2020-03-28-05-31-55-430\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_dir\": \"/opt/ml/input\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CHECKPOINT=/opt/ml/input/data/checkpoint\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{\"checkpoint\":\"/opt/ml/input/data/checkpoint\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"evaluate_steps\":1462},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"checkpoint\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-rl-mxnet-2020-03-28-05-31-55-430\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-28-05-31-55-430/source/sourcedir.tar.gz\",\"module_name\":\"evaluate-coach\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"evaluate-coach.py\"}\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=evaluate-coach\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator\":\"RLEstimator\"}\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATE_STEPS=1462\u001b[0m\n",
      "\u001b[34mSM_HPS={\"evaluate_steps\":1462}\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-031118886020/sagemaker-rl-mxnet-2020-03-28-05-31-55-430/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"checkpoint\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=evaluate-coach.py\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--evaluate_steps\",\"1462\"]\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"checkpoint\"]\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m evaluate-coach --evaluate_steps 1462\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#033[93mWarning: failed to import the following packages - tensorflow#033[0m\u001b[0m\n",
      "\u001b[34mLoading preset preset-portfolio-management-clippedppo from /opt/ml/code\u001b[0m\n",
      "\u001b[34m## Creating graph - name: BasicRLGraphManager\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\u001b[0m\n",
      "\u001b[34m## Creating agent - name: agent\u001b[0m\n",
      "\u001b[34mRequested devices [gpu(0)] not available. Default to CPU context.\u001b[0m\n",
      "\u001b[34mRequested devices [gpu(0)] not available. Default to CPU context.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[34m## Loading checkpoint: /opt/ml/input/data/checkpoint/28_Step-59898.ckpt\u001b[0m\n",
      "\u001b[34m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=1, Total reward=3035.38, Steps=506, Training iteration=0\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=2, Total reward=3071.71, Steps=1012, Training iteration=0\u001b[0m\n",
      "\u001b[34mTesting> Name=main_level/agent, Worker=0, Episode=3, Total reward=3007.65, Steps=1518, Training iteration=0\u001b[0m\n",
      "\u001b[34m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 3038.25\u001b[0m\n",
      "\u001b[34m2020-03-28 05:35:21,698 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-03-28 05:35:33 Completed - Training job completed\n",
      "Training seconds: 63\n",
      "Billable seconds: 63\n",
      "CPU times: user 1.41 s, sys: 35.9 ms, total: 1.45 s\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator_eval = RLEstimator(role=role,\n",
    "                      source_dir='src/',\n",
    "                      dependencies=[\"common/sagemaker_rl\"],\n",
    "                      toolkit=RLToolkit.COACH,\n",
    "                      toolkit_version='0.11.0',\n",
    "                      framework=RLFramework.MXNET,\n",
    "                      entry_point=\"evaluate-coach.py\",\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=\"ml.m4.4xlarge\",\n",
    "                      hyperparameters = {\n",
    "                          \"evaluate_steps\": 731*2 # evaluate on 2 episodes\n",
    "                      }\n",
    "                    )\n",
    "estimator_eval.fit({'checkpoint': checkpoint_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Disclaimer (for live-trading)\n",
    "\n",
    "This notebook is for educational purposes only. Past trading performance does not guarantee future performance. The loss in trading can be substantial, and therefore \n",
    "**investors should use all trading strategies at their own risk**."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
